\chapter{Задача тематического моделирования и методы решения}


\section{Постановка задачи}

Метод тематического моделирования (topic modeling) можно отнести к широкому классу методов обработки текстовых коллекций, основанных на явном или неявном представлении связей между документами и составляющими их терминами в матричном виде (матрица частот встречаемости терминов в документах, которая может быть очень большой, разреженной и шумной) и последующей обработки такой матрицы с целью получения приближенного представления, существенно меньшего по размеру, менее шумного и сохраняющего (а, по возможности, эксплицирующего) полезную информацию, содержащуюся в исходной матрице. Метод тематического моделирования позволяет описать текстовую коллекцию в виде произведения двух матриц, где одна матрица может интерпретироваться как описывающая документы в виде дискретного распределения над фиксированным набором скрытых тем (topic), а вторая как описывающая темы в виде дискретного распределения над терминами. Таким образом, результатом обработки являются плотные (dense) векторные репрезентации документов и описывающих их тем, которые в свою очередь могут быть использованы в большом числе прикладных задач. 

Формально, рассматривается коллекция текстовых документов $D$, множество входящих в них терминов (слов или словосочетаний) $W$. Каждый документ $d \in D$ рассматривается как последовательность терминов $w_1, \ldots, w_{n_d}$ из словаря $W$, где $n_d$ -- количество терминов в документе $d$. Множество документов может быть рассмотрено как случайная и независимая выборка троек $(w_i, d_i, t_i)$, $i = 1, \ldots, n$ из дискретного распределения $p(w, d, t)$ на конечном вероятностном пространстве $W \times D \times T$, где $t \in T$ -- латентная переменная, представляющая тему. \parencite{vorontsov2015topic}

Метод тематического моделирования опирается на следующие допущения:
\begin{enumerate}
  \item вхождение термина в документ некоторым образом связано с темой;
  \item документы рассматриваются как неупорядоченные наборы терминов, порядок слов и грамматичность предложений игнорируются (гипотеза ``мешка слов'');
  \item порядок текстов в коллекции не важен (гипотеза ``мешка документов'');
  \item встречаемость терминов в документах зависит от темы, но не зависит от документа, т.е. описывается общим для всех документов распределением $p(w|t)$ (гипотеза условной независимости). Это допущение имеет следующие эквивалентные представления: $p(w|d,t) = p(w|t)$, $p(d|w,t) = p(d|t)$, $p(d, w|d) = p(d|t)p(w|t)$.
\end{enumerate}

Порождение документов на основе множества латентных тем может быть формализовано с помощью следующей генеративной модели: $p(w|d) = \sum_{t \in T}p(w|t)p(t|d)$. Эта модель выражает вероятность появления термина в документе через известные распределения $p(w|t)$ и $p(t|d)$. Построение тематической модели является обратной задачей: здесь по известной текстовой коллекции $D$ требуется найти распределения $p(w|t)$ и $p(t|d)$. В общем виде задача определяется как оптимизационная задача стохастического матричного разложения, состоящая в нахождении двух матриц $\Phi = (\phi_{wt})_{W \times T}$ (матрицы терминов тем) и $\Theta = (\theta_{td})_{T \times D}$ (матрицы тем документов), таких что матрица частот терминов в документах $F=(f_{wd})_{W \times D}, f_{wd} = n_{dw} / n_d$ аппроксимируется произведением $F \approx \Phi \Theta$. При этом матрицы $\Phi$ и $\Theta$ рассматриваются в качестве параметров модели, и все три матрицы $F$, $\Phi$ и $\Theta$ являются \textit{стохастическими}, т.е. столбцы $f_d, \phi_t$ и $\theta_d$ представляют собой дискретные распределения (матрицы имеют неотрицательные компоненты и нормированы по столбцам).

В общем виде задача построения тематической модели является некорректно поставленной, т.к. допускает бесконечное множество решений. Для получения устойчивого решения требуется введение ряда дополнительных критериев и ограничений (например, предположение о том, что столбцы $\theta_d$ и $\phi_t$ являются случайными векторами, порождаемыми распределением Дирихле, в модели латентного размещения Дирихле). Число и разнообразие таких критериев, а также различные варианты используемых методов оценки параметров, определяют значительное число вариантов практических реализаций данного подхода. В числе особенностей метода, имеющих важное значение для практики, следует отметить произвольный выбор числа извлекаемых тем пользователем (хотя существуют методы, позволяющие устранять коррелированные и малоинформативные темы), возможные затруднения с интерпретируемостью извлекаемых тем.

Приложения тематического моделирования включают информационный поиск (information retrieval), в особенности разведочный поиск (exploratory search), выявление трендов в новостях и научных публикациях, анализ социальных сетей, классификацию и категоризацию документов, суммаризацию, сегментацию текстов, тегирование веб-страниц и новостей, обнаружение спама, разработку рекомендательных систем и пр. \parencite{vorontsov2015topic}


\section{Методы оценки качества результатов}

Методы оценки качества результатов тематического моделирования можно разделить на внутренние (intrinsic) и внешние (extrinsic). В первом случае речь идет об оценке непосредственно модели с точки зрения её характеристик, осуществляемой по исходной текстовой коллекции. В числе метрик такого рода: перплексия (perplexity), разреженность матриц $\Phi$ и $\Theta$ (количество компонентов, близких к нулю), чистота и контрастность тем, когерентность наиболее вероятных слов темы (общепринятая мера интерпретируемости темы), доля фоновых слов и др. Во втором случае оценка осуществляется с точки зрения качества целевой системы, в которой результаты тематического моделирования используются как составной компонент (например, оценка качества информационного поиска с привлечением ассессоров).

Далее кратко опишем распространенные внутренние оценки.

\subsection*{Перплексия}

Перплексия (perplexity) традиционно используется для характеристики языковых моделей\footnote{Языковая модель -- это распределение $p(w), w \in L \subseteq \Sigma^*$, где $\Sigma^*$ -- множество всевозможных последовательностей букв из конечного алфавита $\Sigma$.}
и характеризует несоответствие модели $p(w)$ наблюдаемому слову $w$. Определяется через логарифм функции правдоподобия и применительно к задаче тематического моделирования формулируется следующим образом: $\mathcal{P}(D; p) = exp(-\frac{1}{n}L(\Phi, \Theta)) = exp(-\frac{1}{n}\sum_{d \in D}\sum_{w \in W}n_{dw}ln\,p(w|d))$, где $n$ -- число терминов в коллекции. Модель $p$ тем лучше предсказывает термины в документах коллекции, чем меньше перплексия.

При обучении тематической модели на всей коллекции и вычислении перплексии на той же обучающей выборке, перплексия оказывается занижена (т.е. оказывается лучше, чем должна быть) в силу переобучения. Чтобы этого избежать, рассматривают перплексию контрольной выборки (hold-out perplexity), т.е. считают на части коллекции, не использовавшейся для обучения (например, отбирая случайным образом десятую часть документов из исходной коллекции).

Перплексия как метрика обладает определенными недостатками, т.к. её значение зависит не только от качества модели, но и от длины документов, размера словаря и т.п.

\subsection*{Когерентность}

Когерентность (согласованность) темы определяется на основе поточечной взаимной информации (pointwise mutual information) и характеризует совместную встречаемость наиболее вероятных терминов определенной темы в документах коллекции (использовавшейся для обучения или сторонней).

$PMI(t) = \sum_{i=1}^{k-1}\sum_{j=i+1}^{k}log\frac{N(w_i, w_j)}{N(w_i)N(w_j)}$, где $w_i$ - i-й термин в порядке убывания $\phi_{wt}$, $N(w)$ -- число документов, содержащих термин $w$, $N(w, w')$ -- число документов, в которых термины $w$ и $w'$ встречаются в окне фиксированного размера, $k$ -- ограничение на число наиболее вероятных терминов, используемых для расчета, на практике обычно равно 10.

Средняя когерентность тем применяется в качестве меры интерпретируемости тематической модели.

\subsection*{Разреженность}

Разреженность -- доля ненулевых элементов в матрицах $\Phi$ и $\Theta$ (либо только по предметным темам, если в модели различаются предметные и фоновые темы).

\subsection*{Показатели интерпретируемости темы на основе лексического ядра}

Множество слов, отличающих одну тему от остальных, называют её лексическим ядром. Наличие ядра позволяет характеризовать тему как интерпретируемую.

Ядро темы $W_t = \{w \in W \; | \; p(t|w) > 0,25\}$, где $p(t|w) = \phi_{wt}\frac{n_t}{n_w}$. Показатели интерпретируемости темы:
\begin{enumerate}
    \item чистота $\Sigma_{w \in W_t}p(w|t)$ (чем выше, тем лучше)
    \item контрастность $\frac{1}{|W_t|\Sigma_{w \ in W_t}p(t|w)}$ (чем выше, тем лучше)
    \item мощность ядра $|W_t|$ (оптимум вблизи $\frac{|W|}{|T|}$)
\end{enumerate} 

Соответствующие показатели для всей модели считаются как среднее по всем предметным темам.

\subsection*{Доля фоновых слов}

$\mathcal{B} = \frac{1}{n}\Sigma_{d \in D}\Sigma_{w \in W}\Sigma_{t \in T}n_{dw}p(t|d,w)$, принимает значения в диапазоне $[0, 1]$, экстремальные значения говорят о вырожденности модели.


\section{Основные традиционные методы (pLSA, LDA)}

Основные традиционные методы: байесовское обучение, LSA, pLSA, LDA. Обзор основных реализаций (Gensim, Vowpal Wabbit, BigARTM и др.)

\section{Аддитивная регуляризация для тематических моделей (ARTM)}

Понятие регуляризации. Аддитивная регуляризация. ARTM. Характеристика метода. Технические аспекты (отсутствие автоматической процедуры подбора регуляризаторов и пр.). Реализация метода в библиотеке BigARTM.

\section{Нейросетевые методы. BERTopic}

Нейросетевые методы. Отказ от допущения ``мешка слов'', попытки учесть контекстные значения. Краткая характеристика основных механизмов трансформерных моделей: вектроные репрезентации значений слов, энкодер-декодер архитектура, механизм внимания, KVQ-формализм, понятие языковой модели и маскированной языковой модели (MLM). Понятие предобученной модели и transfer learning. BERT как энкодер, способный учитывать контекстные значения слов. Тематическое моделирование с помощью архитектуры BERTopic.

\section{Состязательное тематическое моделирование}

Состязательные сети (adversarial). Тематическое моделирование средствами adversarial topic modeling (ATM).



\chapter{Автоматическое именование тем}

\section{Характеристика проблемы интерпретируемости}

Характеристика проблемы интерпретируемости тем (не во всех downstream задачах это является проблемой). В тематическом моделировании документы представляются как вектора, где размерностями выступают темы, а значениями (коэффициентами) по соответствующим размерностям -- вероятности тем. Т.о. это представление является интерпретируемым (в отличие от document embeddings в doc2vec и аналогичных подходах). Однако, число тем и их содержание уже не являются непосредственно интерпретируемыми: число тем задается пользователем (ср. k-NN кластеризация), темы описываются распределением над термами.

Задача интерпретации тем пользователем первоначально решалась либо представлением темы в виде списка наиболее важных слов, либо путем её аннотирования вручную. Впервые задача автоматического именования тем была поставлена в работе \parencite{mei2007automatic} и в дальнейшем стала важной подзадачей направления тематического моделирования. В данном разделе рассмотрим основные подходы к её решению.

\section{Mei, Shen, Zhai. Automatic labeling of multinomial topc models. KDD, 2007.}

Первая работа, в которой ставится задача автоматического именования тем -- \parencite{mei2007automatic}. Здесь предлагается формальная постановка задачи, рассматриваются интуитивные требования к выбору наименований тем и разрабатываются формальные критерии, отражающие различные аспекты интерпретируемости. В этой работе предлагается двухэтапный подход, ставший в дальнейшем основным при решении задачи автоматического именования тем, состоящий в генерации набора  наименований-кандидатов и их последующего ранжирования с целью отбора наилучших по ряду критериев, включающих понятность для пользователя, способность отражать значение темы и возможность отличать одну тему от остальных, опираясь на её наименование.

В тематическом моделировании обнаруживаемые темы представляются в виде мультиномиального распределения на множестве слов (т.е. в виде униграммной языковой модели). Ключевая идея подхода, предлагаемого в статье, состоит в представлении возможных наименований также в виде мультиномиального распределения на множестве слов. Благодаря этому оказывается возможным оценивать сходство двух этих распределений и таким образом свести задачу выбора наилучших наименований к оптимизационной задаче, основанной на минимизации расстояния Кульбака-Лейблера между распределениями темы и наименования.

Среди возможных вариантов выбора кандидатов для наименований, включающих отдельные термины, фразы или предложения, авторы останавливаются на использовании фраз, т.к. фразы отличаются большей согласованностью, чем отдельные термины, достаточно компактны по сравнению с предложениями, но при этом способны отражать значение темы. Кроме того, было замечено, что при ручной разметке, аннотаторы также используют коротки фразы для описания значения тем.

Предлагается следующая формальная постановка задачи автоматического наименования тем:

Тематическая модель $\theta$ текстовой коллекции $\mathcal{C}$ -- это вероятностное распределение на множестве слов $\{p(w|\theta)\}_{w \in V}$, где $V$ -- словарь. Ясно, что $\sum_{w \in V}p(w|\theta) = 1$.

Наименование $l$ для темы $\theta$ -- это осмысленная последовательность слов, отражающая скрытое значение $\theta$.

Оценка релевантности наименования темы $s(l, \theta)$ является характеристикой семантичекого сходства между наименованием и темой. Наименование $l_1$ лучше наименования $l_2$, если $s(l_1, \theta) > s(l_2, \theta)$.

\textit{Задача наименования отдельной темы} $\theta$, извлеченной из текстовой коллекции, состоит в определении множества наименований-кандидатов $L = \{l_1, \ldots, l_m\}$, разработке функции оценки релевантности $s(l_i, \theta)$ и нахождении подмножества $L_\theta = \{l_{\theta, 1}, \ldots, l_{\theta, n}\}$ из $n$ наименований, имеющих наибольшую оценку релевантности  для $\theta$.

Задача наименования отдельной темы обобщается на \textit{множество тем} следующим образом. Пусть $\Theta = \{\theta_1, \ldots, \theta_k\}$ -- множество из $k$ тем, $L = \{l_1, \ldots, l_m\}$ -- множество наименований-кандидатов. Требуется отобрать подмножество из $n_i$ наименований, $L_{\theta_i} = \{l_{\theta_i, 1}, \ldots, l_{\theta_i, n}\}$ для каждой темы $\theta_i \in \Theta$ .

В некоторых практических сценариях применения тематического моделирования набор наименований-кандидатов может быть заранее определен (например, задаваться онтологией). Однако, в общем случае предполагается, что такой набор заранее неизвестен. Его задание опирается на использование некоторой референсной текстовой коллекции, которая может быть внешней по отношению к набору документов, для которых создается тематическая модель, либо совпадать с ним.\footnote{В качестве примера внешней референсной коллекции можно привести статьи с конференции по определенной тематике.}

Общая процедура нахождения наилучших наименований для тем включает три этапа:
\begin{enumerate}
    \item создается набор осмысленных наименований-кандидатов 
    \item разрабатывается функция оценки релевантности наименований относительно темы
    \item производится отбор наименований для удовлетворения интуитивных критериев качества наименования, таких как способность различение тем и адекватное отражение семантики отдельной темы. 
\end{enumerate}

В качестве методов порождения набора наименований-кандидатов рассматриваются неглубокий синтаксический анализ (chunking) и статистические методы отбора n-грамм. Первый метод основан на применении специальных программных решений, извлекающих именные группы с опорой на частеречную разметку, определенный грамматический формализм и т.п. Эти решения сложны, могут требовать дополнительных обучающих данных и доступны для ограниченного числа языков. Второй метод опирается на отбор частотных n-грамм из референсной текстовой коллекции в предположении, что такие n-граммы обычно представляют собой осмысленные фразы. Отбор производится с помощью статистических метрик  (например, взаимной информации) или с помощью методов статистической проверки гипотез ($\chi^2$, $t$-тест Стьюдента). Этот способ применим к текстовым коллекциям в любых предметных областях, однако показывает хорошие результаты только для биграмм.

Для оценки семантической релевантности наименований-кандидатов извлекаемым темам предлагаются две функции.

Функция оценки релевантности \textit{нулевого порядка} позволяет оценивать релевантность наименования теме без использования референсной текстовой коллекции. Обозначим фразу-кандидата $l = u_0 u_1 \ldots u_m$, где $u_i$ -- слово. Оценка $score = log \, \frac{p(l|\theta)}{p(l)} = \sum_{0 \leqslant i \leqslant m} log \, \frac{p(u_i|\theta)}{p(u_i)}$, где $u_i$ предполагаются независимыми. Интуитивный смысл данной функции: фраза, содержащая более важные слова по распределению темы (т.е. с большим $p(w|\theta)$), является хорошим наименованием для данной темы. Здесь вероятность слова $p(u_i)$ в знаменателе требуется для компенсации смещения, связанного с обучающей текстовой коллекцией, и может оцениваться по вспомогательной коллекции или быть принята за равномерную. То есть, с помощью данной функции фраза оценивается на основе правдоподобия её порождения тематической моделью $\theta$ по сравнению с некоторым вспомогательным распределением.

Функция оценки релевантности \textit{первого порядка} основана на представлении наименования с помощью распределения над словами исходной текстовой коллекции, благодаря чему оно может сравниваться с распределением темы по расстоянию Кульбака-Лейблера. Пусть $l$ -- наименование, $\{p(w|l)\}$ -- мультиномиальное распределение, ассоциированное с $l$. Тогда расстояние Кульбака-Лейблера $D(\theta||l)$ отражает то, насколько хорошо $l$ описывает $\theta$.  $D(\theta||l) = 0$ в случае, если эти распределения совпадают. Построение распределения $\{p(w|l)\}$ требует использования референсной текстовой коллекции $\mathcal{C}$, выступающей в качестве контекста. Функция оценки релевантности наименования $l$ относительно темы $\theta$ определяется как отрицательное расстояние Кульбака-Лейблера между распределениями $\{p(w|\theta)\}$ и $\{p(w|l)\}$. За счет введения контекста $\mathcal{C}$, функция оценки релевантности может быть преобразована следующим образом:

$score(l, \theta) =$

$= -D(\theta||l) =$

$= -\sum_{w}p(w|\theta)log\,\frac{p(w|\theta)}{p(w|l)} =$

$= -\sum_{w}p(w|\theta)log\,[\frac{p(w|\mathcal{C})}{p(w|l,\mathcal{C})}\cdot  \frac{p(w|\theta)}{p(w|\mathcal{C})}\cdot\frac{p(w|l,\mathcal{C})}{p(w|l)}] =$

$= -\sum_{w}p(w|\theta)log\,\frac{p(w|\mathcal{C})}{p(w|l,\mathcal{C})} -\sum_{w}p(w|\theta)log\,\frac{p(w|\theta)}{p(w|\mathcal{C})} -\sum_{w}p(w|\theta)log\,\frac{p(w|l,\mathcal{C})}{p(w|l)} =$

$= \sum_{w}p(w|\theta)\,PMI(w,l|\mathcal{C}) - D(\theta||\mathcal{C}) + Bias(l, \mathcal{C})$

\noindent Здесь второй компонент представляет собой расстояние Кульбака-Лейблера между темой и \textit{контекстом}. Т.к. он не зависит от наименований, в функции оценки релевантности наименований он может быть опущен. Третий компонент может рассматриваться как смещение (bias) при использовании контекста для оценки семантической релевантности $l$ и $\theta$. Наконец, первый компонент представляет собой матожидание поточечной взаимной информации (PMI) между $l$ и терминами темы при заданном контексте: $E_\theta(PMI(w,l|\mathcal{C}))$.

При отсутствии априорных данных о смещении, определяемом контекстом (т.е. референсной текстовой коллекцией), функция оценки релевантности может быть редуцирована до $s(l,\theta) = E_\theta(PMI(w,l|\mathcal{C}))$. Значения $E_\theta(PMI(w,l|\mathcal{C}))$ могут считаться независимо от тем. В случае, если определенное слово из обучающей текстовой коллекции не содержится в референсной коллекции, $PMI(w,l|\mathcal{C})$ не определена. Тогда соответствующее слагаемое может полагаться равным нулю, либо может применяться та или иная стратегрия сглаживания (например, сглаживание Лапласа). 

Данная функция берется в качестве искомой функции оценки релевантности первого порядка и используется для ранжирования наименований-кандидатов. Интуитивно, она тем выше ранжирует наименование, чем сильнее его семантическая связь с наиболее важными (вероятными) словами темы.

Заключительная часть процедуры нахождения наилучших наименований тем состоит в отборе таких наименований, которые удовлетворяют интуитивным критериям качества. 

Во-первых, необходимо, чтобы наименования максимально отражали семантическую информацию конкретной темы. Поскольку для описания темы может применяться более одной фразы, необходимо последовательно максимизировать информативность каждого следующего наименования с учетом уже выбранных. Это достигается путем поочередного отбора наименований из числа кандидатов, при этом на каждом шаге максимизируется критерий максимальной маржинальной релевантности (maximal marginal relevance, MRR):

\[\hat{l} = \argmax_{\l \in L\setminus S} \, [\lambda Score(l, \theta) - (1-\lambda)\max_{l' \in S} Sim(l', l)], \]

\noindent где $S$ -- множество уже отобранных наименований, $Sim(l', l) = -D(l'||l) = -\Sigma_w p(w|l') log \frac{p(w|l')}{p(w|l)}$, $\lambda$ -- (гипер)параметр.

Во-вторых, требуется отбирать наименования, способные наилучшим образом различать темы в случае, если наименования назначаются более чем одной теме, т.к. в этом случае наименование с высокой оценкой одновременно для нескольких тем оказывается бесполезно. Т.е. необходимо обеспечить высокую оценку для одной темы и низкую для остальных. Это достигается следующим образом:

\[Score'(l, \theta_i) = Score(l, \theta_i) - \mu\,Score(l, \theta_{1,\ldots, i-1,i+1, k}),\]

\noindent где $\mu$ -- параметр, регулирующий степень различающей способности критерия, $Score(l, \theta_{1,\ldots, i-1,i+1, k}) \approx (1 + \frac{\mu}{k-1})E_{\theta_i}(PMI(w,l|\mathcal{C})) - \frac{\mu}{k-1}E_{\theta_j}(PMI(w,l|\mathcal{C}))$. Используя эту функцию, можно добиться необходимого различения тем.


\section{Mirzagitova, Mitrofanova. Automatic assignment of labels in topic modelling for Russian corpora. 2016}

Подход к автоматическому именованию тем, разрабатываемый в данной работе, основан на использовании графового представления связей между терминами. Алгоритм двухэтапный: сначала производится порождение кандидатов осуществляется с помощью алгоритма PageRank и набора морфологических фильтров, затем производится ранжирование и отбор. Работа опирается на предшествующие разработки в области именования тем с использованием графовых методов, описанные в \parencite{aletras2014labelling}. Особенностью рассматриваемой работы является обработка русского языка и использование специализированного русскоязычного корпуса для оценки метода.

На этапе порождения кандидатов из списка слов отбираются 10 наиболее вероятных слов определенной темы. Для каждого из этих слов выполняется запрос в поисковой системе. На странице поисковой выдачи отбираются 30 заголовков, начиная с первой позиции, которые объединяются в один текст. Этот текст подвергается токенизации и лемматизации. Из полученных словоупотреблений строится граф, в котором леммы располагаются в вершинах графа, ребра проставляются между теми леммами, которые встречаются в тексте в окне размера [-2, +2]. Ребра взвешиваются тремя способами: 
\begin{itemize}
    \item[(I)] все веса равны 1; 
    \item[(II)] веса равны частоте совместной встречаемости соответствующих лемм в пределах данного текста;
    \item[(III)] веса равны поточечной взаимной информации (PMI), рассчитанной по русскоязычному разделу Википедии.
\end{itemize} 
\noindent Затем к полученному взвешенному графу применяется PageRank (PageRangk вычисляется для каждой вершины). Таким образом, в результирующем графе вершинам и ребрам присваиваются веса, причем вершины (леммы) взвешиваются по ``важности'', а ребра (биграммы) взвешиваются по ``семантической связанности''. Из полученного графа отбираются n-граммы (подграфы) по следующим морфологическим шаблонам:
\begin{itemize}
    \item[] прилагательное + существительное
    \item[] существительное + существительное в родительном падеже
    \item[] существительное + предлог + существительное
    \item[] существительное + союз + существительное
    \item[] и т.п.
\end{itemize} 
Контактирующие фразы конкатенируются и добавляются в качестве дополнительных кандидатов.

Для этапа ранжирования рассматриваются три ранжирующих метрики
\begin{itemize}
    \item[(A)] сумма весов слов во фразе-кандидате  
    \item[(B)] сумма весов, отнормированная по длине фразы
    \item[(C)] сумма коэффициентов, домноженная на $1 + \frac{1}{i}$, где $i$ -- ранг слова, отобранного из числа десяти наиболее вероятных терминов по распределению темы, включенного в исходный запрос.
\end{itemize} 

Для оценки метода использовался корпус русскоязычных энциклопедических текстов о лингвистике, размером 1900 докуменов, 1,3 млн. словоупотреблений. Из корпуса были удалены стоп-слова, произведена лемматизация с помощью библиотеки pymorphy2, результарующая обучающая текстовая коллекция содержит около 800 тыс. словоупотреблений. Для извлечения тем использовалась реализация латентного размещения Дирихле из библиотеки scikit-learn. Производилось извлечение двадцати тем. Темам были автоматически присвоены наименования в соответствии с описанной процедурой. Результаты были размечены вручную (оценки релевантности от 0 до 3, где 0 -- нерелевантно). В качестве baseline использовалось наиболее вероятное слово по распределению темы. 

Результаты оценки: baseline 1.03, лучший вариант A-III (ранжирование на основе простой суммы весов, взвешивание графа по PMI на основе Википедии) 2.07, все остальные варианты лучше baseline. При этом схема A-II (использование частоты совместной встречаемости в текстах исходной обучающей коллекции) также показала приемлемый результат, при этом данная схема вычислительно проще и не требует использования внешнего корпуса. 


\section{Lishan Cui, Xiuzhen Zhang, Amanda Kimpton, Daryl D'Souza. Automatic labelling of topics via analysis of user summaries. 2016}

В данной работе представлен доход к извлечению слов и осмысленных фраз, используемых в задаче автоматического именования тем, из внешних пользовательских обзоров и комментариев. Пользовательские обзоры, представленные на сайтах Yahoo! Answers, Stack Overflow, или пользовательские комментарии к новостям, описаниям товаров в интернет-магазинах и др., отличаются высокой релевантностью относительно связанных с ними документов, тогда как генерация наименований-кандидатов по таким внешним источникам, как Википедия, не всегда приводит к осмысленным результатам для текстов узкой предметной области.

Предлагаемая процедура автоматического именования тем включает следующие шаги:
\begin{itemize}
    \item с помощью парсера для грамматик зависимостей с последующей фильтрацией по критерию частотности для отсечения шума из пользовательских обзоров генерируются осмысленные фразы, которые далее используются в качестве кандидатов для наименований тем;
    \item на основе расстояния Кульбака-Лейблера строится ранжирующая функция для оценки семантической ассоциации между темой и фразами-кандидатами, причем темы и фразы-кандидаты представляются как распределения над документами, а не словами.
\end{itemize} 

На первом шаге осуществляется обработка коллекции пользовательских обзоров для формирования множества кандидатов. Непосредственно отбираются фразы длиной до трех слов и встречающиеся в корпусе не менее четырех раз (эмпирическое наблюдение: такие короткие фразы обычно представляют собой отдельные существительные или именные группы). Предложения длиннее трех слов обрабатываются с помощью стенфордского парсера для грамматик зависимостей (Stanford dependency parser). Из полученной синтаксической структуры выбираются подструктуры, содержащие существительное в качестве вершины и зависимые узлы, в линейной последовательности непосредственно примыкающие к вершине, т.к. такие подструктуры с большой вероятностью представляют собой именные группы. Дополнительно отфильтровывается шум по порогу частотности (количество употреблений на число пользовательских обзоров <1\%).

Формальная постановка задачи автоматического именования в целом соответствует постановке из работы \parencite{mei2007automatic}, однако имеется важное отличие. Так как наименование-кандидат в рассматриваемом подходе всегда ассоциировано с документом, то и для $l$, и для $\theta$ оказывается возможным определить распределение над документами (одними и теми же). Поэтому функцию оценки релевантности можно задать как $s(l, \theta) = -D_{KL}(P_\theta||P_l) = -\Sigma_{d \in D}P_\theta(i)\,ln\frac{P_\theta(d)}{Q_l(d)}$, где $P_\theta$ -- мультиномиальное распределение на множестве документов для темы $\theta$, $Q_l$ -- мультиномиальное распределение на множестве документов для кандидата $l$, $d \in D$ -- документ. По ранжирующей функции $s(l, \theta)$ отбирается несколько кандидатов с максимальной оценкой (в данной работе -- два), которые используются в качестве искомого наименования темы.

Экспериментальная оценка метода осуществлялась по корпусу, сформированному на основе веб-форума о медицинских услугах Patient Opinion Australia. Корпус включает 623 текста и 593 пользовательских обзора, средняя длина обзора составляет 4 слова, максимальная длина обзора -- 29 слов. С помощью непераметрического LDA было извлечено 50 тем, 9 тем отфильтрованы как несогласованные (фильтрация осуществлялась на основе наличия частотных фраз, которые были включены в тему во фрагментарном виде). Оценка выполнялась ассессорами по шкале от 0 (нерелевантно) до 3 (очень хорошо). Предложенный метод сравнивался с альтернативами: 
\begin{itemize}
    \item использование в качестве наименования десяти наиболее вероятных слов темы;
    \item использование в качестве наименования десяти вероятных слов с их переранжированнием на основе частоты встречаемости в пользовательских обзорах;
    \item использование в качестве наименования одной тысячи наиболее частотных биграмм из документов.
\end{itemize} 
\noindent Предложенный метод получил оценку 1.815, альтернативные варианты -- оценку в диапазоне от 1.32 до 1.36.

 






\chapter{Автоматическое определение однородности}

\section{Kilgarriff. Using Word Frequency Lists to Measure Corpus Homogeneity and Similarity between Corpora. 1997}

В работе предлагается количественная метрика для сравнения двух корпусов и оценки однородности корпуса, использующая частотные списки слов и основанная на вычислении $\chi^2$.

Оценка однородности корпуса является необходимым предварительным этапом при количественной оценке схожести двух корпусов, т.к. неясно, как интерпретировать результаты сравнения однородного корпуса не неоднородным. Традиционно для оценки однородности корпуса используется перплексия, расширением этого метода для задачи сравнения корпусов является вычисление кросс-энтропии. В данной работе предлагается альтернативный метод, применимый одновременно к обеим задачам. Метод вычисления (в случае оценки однородности) состоит в следующем:
\begin{itemize}
    \item корпус случайным образом делится на две части
    \item для каждого из подкорпусов создаются частотные списки слов
    \item разница между подкорпусами оценивается с помощью вычисления $\chi_2$
    \item нормирование
    \item итерирование для получения других случайных подкорпусов
    \item интерпретация результата путем сравнения значений, полученных для различных корпусов
\end{itemize}
Случай сравнения корпусов отличается тем, что подкорпуса выбираются из двух сравниваемых корпусов, а значение сходства интерпретируется относительно меры однородности каждого из корпусов.

Обоснованием применимости частотных списков слов для оценки лингвистического содержание текстов и корпусов служит следующее утверждение: любые языковые отличия находят своё отражение в отличиях частотности слов. 

% Поначалу кажется, что статистический тест $\chi^2$ позволяет обнаружить, что два корпуса получены из одного распределения (нулевая гипотеза), либо что распределения двух или нескольких языковых явлений в этих корпусах статистически значимо различаются. Для таблицы сопряженности размерности $m \times n$, при условии верности нулевой гипотезы, статистика $\sum_{i = 1, \ldots, m, j=1,\ldots,n}\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$, где $O_{ij}$ -- наблюдаемое значение, $E_{ij}$ -- ожидаемое значение, вычисленное на основе объединенного корпуса, распределена в соответствии с $\chi^2$-распределением с $(m-1) \times (n-1)$ степенями свободы.



\section{Cavaglia. Measuring corpus homogeneity using a range of measures for inter-document distance. LREC, 2002.}

Задача работы -- численная характеристика корпуса, допускающая сравнение корпусов. Любому кличественном сравнению корпусом должен предшествовать анализ однородности (corpus homogeneity). В этой работе описывается метод анализа текстов, основанный исключительно на внутритекстовых лингвистических признаках, и набор связанных друг с другом мер однородности, основанных на расстоянии между документами. Представлен предварительный эксперимент, имеющий целью проверку гипотезы, что для однородного корпуса подкорпус, необходимый для обучения NLP-систем, требуется меньшего размера, чем в случае с неоднородным корпусом.

Результаты, полученные с использование определённого корпуса, зачастую не могут быть обобщены на корпус большего размера. Без внесения существенных ошибок (bias) результаты могут быть распространены только на корпуса, собранные с учетом явных критериев, таких как репрезентативность выборки. Отсюда возникают задачи описания корпуса, сравнения нескольких корпусов и создания новых корпусов, являющихся репрезентативными для определенных языковых вариантов. Для описания корпусов используют внешние и внутренние критерии. К внешним относятся преимущественно нелингвистические характеристики, не содержащиеся непосредственно в текстах корпуса, такие как тема документа, жанр, возраст и род занятий автра и пр. Такие критерии обычно описываются вручную. Внутренние критерии основаны на признаках текстов, и могут характеризовать словник, части речи и пр. Внутренние критерии представляются более объективными и могут быть расчитаны автоматически. 

В данной работе рассматриваются внутренние критерии описания и сравнения корпусов, причем рассматриваются только критерии, относящиеся к внутритекстовой информации: лексическим, семантическим и синтаксическим аспектам. Корпус называется однородным (homogenous), если внутренние признаки входящих в него документов существенно не различаются. Следуя определению Килгариффа, сходство корпусов (corpus similarity) определяется как характристика возможности распространения лингвистических наблюдений, сделанных на основе одного корпуса, на другой корпус.

В работе предлагается стохастический метод описания и сравнения корпусов, основанных на их внутренних признаках. Метод включает следующие этапы: определение исследуемого аспекта и используемых признаков, сбор данных для каждого документа в корпусе, вычисление метрики сходства для каждой пары документов, вычисление общей количественной характеристики корпуса на основе попарных сравнений.

Для сбора признаков используются методы лексического анализа: в зависимости от исследуемого аспекта (лексического, синтаксического) могут отбираться непосредственно словоформы, леммы, служебные слова, частречные пометы и пр. Далее каждый текст представляется в виде частотного списка используемых признаков. Частотный список -- это список пар $(x, f(x))$, где $x$ -- признак (например, определенное служебное слово), $f(x)$ -- число вхождений данного признака в документ (например, число употреблений данного служебного слова). Значения $f(x)$ нормализуются по всему корпусу, чтобы в дальнейшем перейти к рассмотрению пар $(x, p(x))$, где $p(x)$ -- вероятность признака $x$ в документе. В результате корпус представляется в виде матрицы распределений признаков по всем документам корпуса.

Два документа считаются схожими (similar), если схожи их распределения. В работе исследуются три различных меры сходства: расстояние Кульбака-Лейблера и две меры, оценивающие отклонение от нулевой гипотезы, что два документа случайно отобраны из одного распределения: $\chi^2$ и логарифмическая функция правдоподобия.

Если $p(i)$ и $q(i)$ -- распределения, характеризующие два документа, расстояние Кульбака-Лейблера вычисляется следующим образом: $D(p||q) = \sum_{i=1}^{n}p(i)\,log\frac{p(i)}{q(i)}$. Т.к. это выражение не определено при $q(i)=0$, производится сглаживание путем прибавления центроида (усредненного распределения по всему корпусу) к каждому распределению при вычислении оценки схожести: $D'(p||q) = \sum_{i=1}^{n}(p(i)+c(i))\,log\frac{p(i)+c(i)}{q(i)+c(i)}$, где $c(i)$ -- центроид.

Мера сходства на основе $\chi^2$ расчитывается следующим образом. Для каждого признака из частотного списка вычисляется ожидаемое число наблюдений в каждом из документов. Пусть размеры документов $A$ и $B$ равны $N_A$ и $N_B$ соответственно, признак $w$ наблюдается с частотой $o_{w,A}$ в документе $A$ и $o_{w,B}$ в документе $B$, тогда ожидаемое значение $e_{w,A} = \frac{N_A(o_{w,A}+o_{w,B})}{N_A+N_B}$ для $A$ (для $B$ аналогично). Значение $\chi^2$ для пары документов $A$ и $B$ равно $\chi^2 = \sum_{i=1}^{n}\frac{(o_i-e_i)^2}{e_i}$, где суммирование проводится по всем признакам.

Логарифмическая функция правдоподобия $G^2$, см. \parencite{dunning1993accurate}.

Оценки сходства по парам документов сводятся в одну матрицу сходства. На основании этой матрицы вычисляется общие численные оценки для корпуса:
\begin{itemize}
    \item мера однородности, характеризующая изменчивость признака в пределах корпуса, и равная максимальному расстоянию между документами корпуса;
    \item прототипический документ в корпусе, т.е. ближайший к центроиду;
    \item метрика сходства двух корпусов, равная расстоянию между их центроидами.
\end{itemize}

Проводилась экспериментальная оценка трех рассматриваемых мер сходства корпусов по схеме внешней (extrinsic) оценки. На различных корпусах оценивалось качество внешней системы (классификатора текстов) по метрике accuracy и вычислялась её корреляция с метрикой однородности для соответствующего корпуса с использованием коэффициента ранговой корреляции Спирмана. Результаты оценки продемонстрировали, что однородность корпуса существенна для классификатора. Для достижения высокой accuracy классификатора при обучении на однородном корпусе требуется обучающая выборка меньшего размера, чем при обучении на неоднородном корпусе.


\section{Gledson. Measuring Topic Homegeneity and its Application to Dictionary-Based Word Sense Disambiguation.}

В данной статье предложена численная мера однородности темы\footnote{В данной работе понятие темы (topic) относится к информационной структуре текста (``то, о чём говорится в тексте'') и отличается от понятия темы в тематическом моделировании (мультиномиальное распределение на множестве слов).} (topic homogeneity), рассчитываемая на основе ряда лексических ресурсов и не требующая предварительного снятия лексической омонимии. Производится оценка использования наборов тематических признаков различной степени однородности в задаче снятия лексической неоднозначности.

Тематические признаки представляют собой неупорядоченные наборы слов, зачастую представляющие собой контекст употребления целевого слова или фразы. В работе утверждается, что эффективность таких признаков в различных задачах обработки текстов зависит от степени однородности темы. Особенностями рассматриваемой работы, отличающей её от предшествующих исследований, является учет однородности тематических признаков в пределах текста (а не фрагмента текста, содержащего целевое слово или фразу), переход от попарных сравнений к учету лексических цепочек и групп размером до 10 слов.

В экспериментах, представленных в работе, тексты разбиваются на блоки размером до 50 слов без учета границ тем, что позволяет компенсировать неоднородность эффекты, связанные с размерами текстов. Метрики однородности рассчитываются поблочно. Тексты выбираются случайным образом из корпусов Semcor (всего отобран 1040 текст) и SENSEVAL 2, 3 (всего 73). Текстовые блоки подвергаются препоцессингу для удаления ``нетематических'' слов, которые определяются по порогу частотности (встречаемость более чем в 25\% текстов в корпусе Semcor) и наличию пометы \textit{factotum} в WordNet Domains 3.2. Указанная внешняя информация о принадлежности слов к классу нетематических в экспериментах непосредственно не используется. Вместо этого для определения класса применяется вспомогательный классификатор на основе решающего дерева, использующий в качестве обучающей коллекции все тексты Semcor и SENSEVAL 2 и 3. Обученный классификатор показал accuracy 83\% на кросс-валидации с разбиением на 10 частей. Кроме того, в экспериментах рассматривались только существительные и глаголы, т.к., по мнению авторов, они с большей вероятностью отражают тематическую информацию.

В работе предлагаются пять метрик однородности, основанные на разных подходах. Каждая метрика принимает в качесте аргумента предобработанный текстовый блок и возвращает одно число.

1. \textit{Лексическая энтропия} (word entropy) определяется как $Entropy(d) = -\sum_{i = 1}^{n}p(x_i)\,log_2 p(x_i)$, где $n$ -- число различных тематических лемм в документе $d$, $p(x_i) = frequency(lemma_i) / \sum_{j=1}^{n} frequency(lemma_j)$. Поскольку $n$ различается в зависимости от документа, $Entropy(d)$ дополнительно нормализуется путем деления на максимально возможное значение $Entropy$ для документа $d$, которое достигается при равной частотности всех лемм.

2. \textit{Метрики сходства на основе WordNet} рассчитываются с помощью программного пакета WordNet::Similarities 1.04. Для каждого документа рассчитываются три метрики однородности по следующей схеме: 1) тематические леммы входного текстового блока ражируются по убыванию частоты, затем по очередности появления в тексте; 2) из этого списка отбираются до десяти первых лемм; 3) рассчитыватеся средняя метрика сходства по всем парам в полученном сокращенном списке с учетом всех возможных частеречных комбинаций в случае неоднозначности. Данная процедура производится с тремя метриками, представленными в программном пакете, а именно: пересечение толкований WordNet (gloss overlap), информационное содержание наименьшего обобщающего синсета (least common subsumer), длина пути в графе синсетов (path lengths). 

3. \textit{Метрика однородности на основе поисковой выдачи} (поисковик Yahoo) рассчитывается на основе числа найденных документов для каждой леммы из сокращенного списка, полученного аналогично шагам 1-2 для метрики на основе WordNet. Далее по каждым возможным попарным сочетаниям лемм формируются расширенные запросы к поисковой системе длиной два и более ключевых слова с учетом лемм, имеющих более высокий ранг в сокращенном списке. Число найденных документов используется для расчета финальной метрики.

4. \textit{Метрики однородности, рассчитанная с помощью пакета WordNet Domains.} Этот пакет расширяет синсеты WordNet путем дополнительного аннотирования пометами из таксономии предметных областей, содержащей около 200 записей. На основании данных о частотности предметных областей в рассматриваемом документе, рассчитываются две метрики: энтропия для предметных областей по формуле $Entropy(d) = -\sum_{i = 1}^{n}p(x_i)\,log_2 p(x_i)$, где $n$ -- число различных предметных областей в документе, и процентная доля трех наиболее частых предметных областей в документе.

5. \textit{Метрика на основе лексических цепочек}, т.е. кластеров семантически связанных слов. Семантические связи между словами определяются с помощью внешних лексических ресурсов, таких как WordNet или тезаурус Роже. Метрика рассчитывается на основе жадного алгоритма с использованием WordNet путем обхода синсетов по отношениям гиперогипонимии и холонимии с присвоением различных весов в зависимости от типа отношения.

Оценка предложенных метрик осуществляется по схеме внешней оценки (exptrinsic evaluation), где основной внешней задачей выступает снятие лексической неоднозначности. Для оценки применяются три метода. Во-первых, каждая метрика сопоставляется с эквивалентной метрикой, рассчитанной на леммах со снятой семантической неоднозначностью. Во-вторых, с помощью таксономии предметных областей WordNet Domains формируются множества слов различной степени тематической однородности, на которых тестируются метрики; полученные результаты сопоставляются с ожидаемыми. В-третьих, каждая метрика оценивается по её способности предсказывать результаты метрик лексической неоднозначности, вычисляемых на основе совместной встречаемости или с опорой на словарные данные. Эксперименты демонстрируют хорошую корреляцию значений метрик и их комбинаций с ожидаемыми результатами. В задаче снятия лексической неоднозначности учет тематической однородности приводит к лучшим результатам по сравнению с учетом таких факторов, как полисемия и смысловая энтропия.



\section{Yi-An Lai et al. Diversity, Density and Homogeneity. 2020}

При описании текстовых коллекций простые описательные статистики, такие как число слов или размер словаря, не позволяют полноценно охарактеризовать синтаксические и семантические свойства. В данной работе предлагаются метрики разнообразия (diversiry), плотности (density) и однородности, количественно характеризующие дисперсию, разреженность и единообразие текстовой коллекции. В качестве базовых текстовых единиц в данном анализе рассматриваются фразы, предложения и параграфы. Текстовая коллекция отображается в векторное пространство большой размерности. Характеризующие её метрики вычисляются на основе полученных векторных представлений. Для получения векторных репрезентаций предложений используется модель BERT. 

Рассматривается текстовая коллекция $\{x_1, \ldots, x_m\}$, где $x_i$ -- последовательность токенов $x_{i1}, x_{i2}, \ldots, x_{il}$, представляющая собой фразу, предложение или параграф. Метод вложения в векторное пространство $\mathcal{E}$ преобразует $x_i$ в вектор $\mathcal{E}(x_i) = e_i$, метрики вычисляются на основе полученных векторных представлений. Размерность полученных векторов обычно првеосходит 300. Далее, производится группировка множества векторных представлений, которая может выполняться как с учителем (классификация), так и без учителя (кластеризация). В общем случае, предлагаемые метрики не зависят от способа группировки векторов.

\textit{Разнообразие (diversity).} Векторные представления коллекции текстов $\{e_1, \ldots, e_m\}$ могут рассматриваться как кластер в многомерном векторном пространстве. Метрика разнообразия оценивает дисперсию такого кластера путем обобщения понятия радиуса. Точнее, если точки кластера распределены по многомерному нормальному закону с диагональной ковариционной матрицей $\Sigma$, форма изоконтура такого кластера представляет собой эллипсоид в $\mathbb{R}^H$, ориентированный вдоль координатных осей. Такой изоконтур описывается формулой:

\[(\bm{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu}) = \sum_{j=1}^{H}\frac{(x_j-\mu_j)^2}{\sigma_j^2} = c^2,\]

\noindent где $\bm{x}$ -- всевозможные точки $\mathbb{R}^H$, лежащие на изоконтуре, $c$ -- константа, $\bm{\mu}$ -- заданный вектор средних значений, $\sigma_j^2$ -- дисперсия по $j$-ой оси. Опираясь на геометрическую интерпретацию, авторы интерпретируют стандартное отклонение $\sqrt{\sigma_j^2}$  как радиус $r_j$ эллипсоида вдоль $j$-ой оси. Метрика разнообразия определяется как геометрическое среднее радиусов по всем осям: 

\[M_{diversity} = \sqrt[H]{\prod_{i=1}^{H}\sigma_i}\]

\textit{Плотность (density).} Метрика плотности характеризует число точек, расположенных в единичном объеме векторного пространства. Чтобы избежать экстремальных значений плотности (нулевой или бесконечной) в силу размерности объемлющего пространства, единица объема определяется через группировку осей по подпространствам размерности $\sqrt{H}$ (``эффективные оси''). Тогда объем кластера аппроксимируется выражением $volume = (r_1 \cdot \ldots \cdot r_{\sqrt{H}})^{\frac{1}{\sqrt{H}}} \cdots (r_{H-\sqrt{H}+1} \cdot \ldots \cdot r_{H})^{\frac{1}{\sqrt{H}}} = (\prod_{i=1}^{H}\sigma_i)^{\frac{1}{\sqrt{H}}}$. Если число точек кластера равно $m$, то метрика плотности

\[M_{density}=\frac{m}{(\prod_{i=1}^{H}\sigma_i)^{\frac{1}{\sqrt{H}}}}\]

\textit{Однородность (homogeneity).} Метрика однородности характеризует, насколько равномерно вектора текстовой коллекции распределены по объемлющему пространству. Метрика строится на основе представления попарных расстояний между векторами в виде полносвязной взвешенной сети. Эта сеть представляется в виде цепи Маркова, для которой вычисляется энтропия, которая в свою очередь нормируется к диапазону $[0, 1]$, полученное значение есть искомое значение однородности. Точнее, расстояние между векторами, принимаемое за вес соответствующего ребра в графе переходов марковской цепи, равно $weight(i,j) = (\sqrt{(\bm{e}_i - \bm{e}_j)\cdot(\bm{e}_i - \bm{e}_j)})^{log(H)}$; вероятность перехода между состояниями $i$ и $j$ равно $p_{i \to j} = \frac{weight(i, j)}{\sum_k weight(i,k)}$; энтропия такой марковской цепи $entropy= -\sum_{ij}v_i \cdot p_{i\to j}\,log\,p_{i\to j}$, где $v_i$ -- стационарное распределение цепи Маркова. Теоретическая верхняя граница энтропии равна $-\sum_{i,j,i\neq j}(\frac{1}{m})\frac{1}{m-1}\,log\frac{1}{m-1} = log(m-1)$. Окончательно,

\[M_{homogeneity} = \frac{-\sum_{ij}v_i \cdot p_{i\to j}\,log\,p_{i\to j}}{log(m-1)}\]

\noindent Интуитивная трактовка однородности: случайное блуждание по графу расстояний позволяет метрически оценить достижимость из произвольной точки всех остальных точек. Если все точки примерно равноудалены друг от друга, значение энтропии и, соответственно, метрики однородности, будет выше, чем при наличии неравномерной удаленности точек друг от друга. 

Экспериментальная верификация эффективности предлагаемых метрик осуществляется с помощью численных экспериментов для двумерных и многомерных векторных пространств. В ходе экспериментов разработанные метрики демонстрируют ряд желательных иинтуитивных свойств, таких как робастность и линейная чувствительность относительно случайных подвыборок. Помимо этого, выполняется оценка скоррелированности значений предлагаемых метрик с результатами текстовой классификации.

\section{Lobodzinski. Jupyter Notebook.}

Некоторым способом корпус разбит на кластеры, каждый из которых содержит семантически схожие тексты. Известно, что каждый кластер похожих текстов более или менее однороден. При этом некоторые кластеры содержат несхожие тексты. Чтобы определить, все ли кластеры содержат схожие тексты, требуется найти кластеры, которые содержат ошибочные тексты. Для нахождения плохих кластеров используется следующая процедура. 

Во-первых, проверяется схожесть текстов внутри кластера с помощью вычисления энтропии множества текстов, входящих в этот кластер. Вычисление энтропии выполняется путем прямого подсчета символов или токенов в последовательности, генерируемой посредством чтения предложений кластера, т.е. оценивается энтропия текстов кластера как информационного источника: $h(X) = \lim_{n \to \infty}H(X_{n+1}|X_1, X_2, \ldots, X_n)$, где $H(X|Y) = -\sum_{y,x}P(X = x, Y = y)
\,log_2(P(X = x | Y = y))$, с соответствующими ограничениями марковского процесса n-го порядка.

Во-вторых, наихудшие кластеры отсекаются с помощью динамически определяемого порога отсечения (используется эвристический метод).