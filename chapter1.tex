\chapter{Задача тематического моделирования и методы решения}


\section{Постановка задачи}

Метод тематического моделирования (topic modeling) можно отнести к широкому классу методов обработки текстовых коллекций, основанных на явном или неявном представлении связей между документами и составляющими их терминами в матричном виде (матрица частот встречаемости терминов в документах, которая может быть очень большой, разреженной и шумной) и последующей обработки такой матрицы с целью получения приближенного представления, существенно меньшего по размеру, менее шумного и сохраняющего (а, по возможности, эксплицирующего) полезную информацию, содержащуюся в исходной матрице. Метод тематического моделирования позволяет описать текстовую коллекцию в виде произведения двух матриц, где одна матрица может интерпретироваться как описывающая документы в виде дискретного распределения над фиксированным набором скрытых тем (topic), а вторая как описывающая темы в виде дискретного распределения над терминами. Таким образом, результатом обработки являются плотные (dense) векторные репрезентации документов и описывающих их тем, которые в свою очередь могут быть использованы в большом числе прикладных задач. 

Формально, рассматривается коллекция текстовых документов $D$, множество входящих в них терминов (слов или словосочетаний) $W$. Каждый документ $d \in D$ рассматривается как последовательность терминов $w_1, \ldots, w_{n_d}$ из словаря $W$, где $n_d$ -- количество терминов в документе $d$. Множество документов может быть рассмотрено как случайная и независимая выборка троек $(w_i, d_i, t_i)$, $i = 1, \ldots, n$ из дискретного распределения $p(w, d, t)$ на конечном вероятностном пространстве $W \times D \times T$, где $t \in T$ -- латентная переменная, представляющая тему. \parencite{vorontsov2015topic}

Метод тематического моделирования опирается на следующие допущения:
\begin{enumerate}
  \item вхождение термина в документ некоторым образом связано с темой;
  \item документы рассматриваются как неупорядоченные наборы терминов, порядок слов и грамматичность предложений игнорируются (гипотеза ``мешка слов'');
  \item порядок текстов в коллекции не важен (гипотеза ``мешка документов'');
  \item встречаемость терминов в документах зависит от темы, но не зависит от документа, т.е. описывается общим для всех документов распределением $p(w|t)$ (гипотеза условной независимости). Это допущение имеет следующие эквивалентные представления: $p(w|d,t) = p(w|t)$, $p(d|w,t) = p(d|t)$, $p(d, w|d) = p(d|t)p(w|t)$.
\end{enumerate}

Порождение документов на основе множества латентных тем может быть формализовано с помощью следующей генеративной модели: $p(w|d) = \sum_{t \in T}p(w|t)p(t|d)$. Эта модель выражает вероятность появления термина в документе через известные распределения $p(w|t)$ и $p(t|d)$. Построение тематической модели является обратной задачей: здесь по известной текстовой коллекции $D$ требуется найти распределения $p(w|t)$ и $p(t|d)$. В общем виде задача определяется как оптимизационная задача стохастического матричного разложения, состоящая в нахождении двух матриц $\Phi = (\phi_{wt})_{W \times T}$ (матрицы терминов тем) и $\Theta = (\theta_{td})_{T \times D}$ (матрицы тем документов), таких что матрица частот терминов в документах $F=(f_{wd})_{W \times D}, f_{wd} = n_{dw} / n_d$ аппроксимируется произведением $F \approx \Phi \Theta$. При этом матрицы $\Phi$ и $\Theta$ рассматриваются в качестве параметров модели, и все три матрицы $F$, $\Phi$ и $\Theta$ являются \textit{стохастическими}, т.е. столбцы $f_d, \phi_t$ и $\theta_d$ представляют собой дискретные распределения (матрицы имеют неотрицательные компоненты и нормированы по столбцам).

В общем виде задача построения тематической модели является некорректно поставленной, т.к. допускает бесконечное множество решений. Для получения устойчивого решения требуется введение ряда дополнительных критериев и ограничений (например, предположение о том, что столбцы $\theta_d$ и $\phi_t$ являются случайными векторами, порождаемыми распределением Дирихле, в модели латентного размещения Дирихле). Число и разнообразие таких критериев, а также различные варианты используемых методов оценки параметров, определяют значительное число вариантов практических реализаций данного подхода. В числе особенностей метода, имеющих важное значение для практики, следует отметить произвольный выбор числа извлекаемых тем пользователем (хотя существуют методы, позволяющие устранять коррелированные и малоинформативные темы), возможные затруднения с интерпретируемостью извлекаемых тем.

Приложения тематического моделирования включают информационный поиск (information retrieval), в особенности разведочный поиск (exploratory search), выявление трендов в новостях и научных публикациях, анализ социальных сетей, классификацию и категоризацию документов, суммаризацию, сегментацию текстов, тегирование веб-страниц и новостей, обнаружение спама, разработку рекомендательных систем и пр. \parencite{vorontsov2015topic}


\section{Методы оценки качества результатов}

Методы оценки качества результатов тематического моделирования можно разделить на внутренние (intrinsic) и внешние (extrinsic). В первом случае речь идет об оценке непосредственно модели с точки зрения её характеристик, осуществляемой по исходной текстовой коллекции. В числе метрик такого рода: перплексия (perplexity), разреженность матриц $\Phi$ и $\Theta$ (количество компонентов, близких к нулю), чистота и контрастность тем, когерентность наиболее вероятных слов темы (общепринятая мера интерпретируемости темы), доля фоновых слов и др. Во втором случае оценка осуществляется с точки зрения качества целевой системы, в которой результаты тематического моделирования используются как составной компонент (например, оценка качества информационного поиска с привлечением ассессоров).

Далее кратко опишем распространенные внутренние оценки.

\subsection*{Перплексия}

Перплексия (perplexity) традиционно используется для характеристики языковых моделей\footnote{Языковая модель -- это распределение $p(w), w \in L \subseteq \Sigma^*$, где $\Sigma^*$ -- множество всевозможных последовательностей букв из конечного алфавита $\Sigma$.}
и характеризует несоответствие модели $p(w)$ наблюдаемому слову $w$. Определяется через логарифм функции правдоподобия и применительно к задаче тематического моделирования формулируется следующим образом: $\mathcal{P}(D; p) = exp(-\frac{1}{n}L(\Phi, \Theta)) = exp(-\frac{1}{n}\sum_{d \in D}\sum_{w \in W}n_{dw}ln\,p(w|d))$, где $n$ -- число терминов в коллекции. Модель $p$ тем лучше предсказывает термины в документах коллекции, чем меньше перплексия.

При обучении тематической модели на всей коллекции и вычислении перплексии на той же обучающей выборке, перплексия оказывается занижена (т.е. оказывается лучше, чем должна быть) в силу переобучения. Чтобы этого избежать, рассматривают перплексию контрольной выборки (hold-out perplexity), т.е. считают на части коллекции, не использовавшейся для обучения (например, отбирая случайным образом десятую часть документов из исходной коллекции).

Перплексия как метрика обладает определенными недостатками, т.к. её значение зависит не только от качества модели, но и от длины документов, размера словаря и т.п.

\subsection*{Когерентность}

Когерентность (согласованность) темы определяется на основе поточечной взаимной информации (pointwise mutual information) и характеризует совместную встречаемость наиболее вероятных терминов определенной темы в документах коллекции, использовавшейся для обучения, или сторонней коллекции.

$PMI(t) = \sum_{i=1}^{k-1}\sum_{j=1}^{k}log\frac{N(w_i, w_j)}{N(w_i)N(w_j)}$, где $w_i$ - i-й термин в порядке убывания $\phi_{wt}$, $N(w)$ -- число документов, содержащих термин $w$, $N(w, w')$ -- число документов, в которых термины $w$ и $w'$ встречаются в окне фиксированного размера, $k$ -- ограничение на число наиболее вероятных терминов, используемых для расчета, на практике обычно равно 10.

Средняя когерентность тем применяется в качестве меры интерпретируемости тематической модели.

\subsection*{Разреженность}

Разреженность -- доля ненулевых элементов в матрицах $\Phi$ и $\Theta$ (либо только по предметным темам, если в модели различаются предметные и фоновые темы).

\subsection*{Показатели интерпретируемости темы на основе лексического ядра}

Множество слов, отличающих одну тему от остальных, называют её лексическим ядром. Наличие ядра позволяет характеризовать тему как интерпретируемую.

Ядро темы $W_t = \{w \in W \; | \; p(t|w) > 0,25\}$, где $p(t|w) = \phi_{wt}\frac{n_t}{n_w}$. Показатели интерпретируемости темы:
\begin{enumerate}
    \item чистота $\Sigma_{w \in W_t}p(w|t)$ (чем выше, тем лучше)
    \item контрастность $\frac{1}{|W_t|\Sigma_{w \ in W_t}p(t|w)}$ (чем выше, тем лучше)
    \item мощность ядра $|W_t|$ (оптимум вблизи $\frac{|W|}{|T|}$)
\end{enumerate} 

Соответствующие показатели для всей модели считаются как среднее по всем предметным темам.

\subsection*{Доля фоновых слов}

$\mathcal{B} = \frac{1}{n}\Sigma_{d \in D}\Sigma_{w \in W}\Sigma_{t \in T}n_{dw}p(t|d,w)$, принимает значения в диапазоне $[0, 1]$, экстремальные значения говорят о вырожденности модели.


\section{Основные традиционные методы (pLSA, LDA)}

Основные традиционные методы: байесовское обучение, LSA, pLSA, LDA. Обзор основных реализаций (Gensim, Vowpal Wabbit, BigARTM и др.)

\section{Аддитивная регуляризация для тематических моделей (ARTM)}

Понятие регуляризации. Аддитивная регуляризация. ARTM. Характеристика метода. Технические аспекты (отсутствие автоматической процедуры подбора регуляризаторов и пр.). Реализация метода в библиотеке BigARTM.

\section{Нейросетевые методы. BERTopic}

Нейросетевые методы. Отказ от допущения ``мешка слов'', попытки учесть контекстные значения. Краткая характеристика основных механизмов трансформерных моделей: вектроные репрезентации значений слов, энкодер-декодер архитектура, механизм внимания, KVQ-формализм, понятие языковой модели и маскированной языковой модели (MLM). Понятие предобученной модели и transfer learning. BERT как энкодер, способный учитывать контекстные значения слов. Тематическое моделирование с помощью архитектуры BERTopic.

\section{Состязательное тематическое моделирование}

Состязательные сети (adversarial). Тематическое моделирование средствами adversarial topic modeling (ATM).



\chapter{Интерпретируемость тематического моделирования}

\section{Характеристика проблемы интерпретируемости}

Характеристика проблемы интерпретируемости тем (не во всех downstream задачах это является проблемой). В тематическом моделировании документы представляются как вектора, где размерностями выступают темы, а значениями (коэффициентами) по соответствующим размерностям -- вероятности тем. Т.о. это представление является интерпретируемым (в отличие от document embeddings в doc2vec и аналогичных подходах). Однако, число тем и их содержание уже не являются непосредственно интерпретируемыми: число тем задается пользователем (ср. k-NN кластеризация), темы описываются распределением над термами. 

\section{Автоматическая разметка тем}

Задача автоматической разметки тем как подзадача тематического меоделирования. Обзор подходов к её решению.

\section{Методы, основанные на графовых представлениях}

Lorem ipsum dolor sit amet

\section{Методы с опорой на пользовательские аннотации и др.}

Lorem ipsum dolor sit amet



\chapter{Автоматическое определение однородности}

\section{Метрики однородности корпуса и темы}

Метрики однородности корпуса (corpus homogeneity) vs. метрики однородности темы (topic homogeneity). Связь метрик однородности корпуса и метрик качества тематического моделирования на примере perplexity. Использование тематических признаков (topical features) слов в задачах снятия лексической неоднозначности (WSD) и др.

\section{BoW}

Метрики однородности корпуса на основе частот слов (статистики на bag-of-words)

\section{Thesauri}

Метрики однородности корпуса на основе онтологий/тезаурусов – анализ родственных слов в иерархии
семантических понятий

\section{Embeddings}

Метрики однородности корпуса на основе расстояний между эмбеддингами текстов

\section{Enthropy}

Метрики однородности корпуса на основе энтропии
