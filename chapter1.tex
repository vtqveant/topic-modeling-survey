\chapter{Задача тематического моделирования и методы решения}


\section{Постановка задачи}

Метод тематического моделирования (topic modeling) можно отнести к широкому классу методов обработки текстовых коллекций, основанных на явном или неявном представлении связей между документами и составляющими их терминами в матричном виде (матрица частот встречаемости терминов в документах, которая может быть очень большой, разреженной и шумной) и последующей обработки такой матрицы с целью получения приближенного представления, существенно меньшего по размеру, менее шумного и сохраняющего (а, по возможности, эксплицирующего) полезную информацию, содержащуюся в исходной матрице. Метод тематического моделирования позволяет описать текстовую коллекцию в виде произведения двух матриц, где одна матрица может интерпретироваться как описывающая документы в виде дискретного распределения над фиксированным набором скрытых тем (topic), а вторая как описывающая темы в виде дискретного распределения над терминами. Таким образом, результатом обработки являются плотные (dense) векторные репрезентации документов и описывающих их тем, которые в свою очередь могут быть использованы в большом числе прикладных задач. 

Формально, рассматривается коллекция текстовых документов $D$, множество входящих в них терминов (слов или словосочетаний) $W$. Каждый документ $d \in D$ рассматривается как последовательность терминов $w_1, \ldots, w_{n_d}$ из словаря $W$, где $n_d$ -- количество терминов в документе $d$. Множество документов может быть рассмотрено как случайная и независимая выборка троек $(w_i, d_i, t_i)$, $i = 1, \ldots, n$ из дискретного распределения $p(w, d, t)$ на конечном вероятностном пространстве $W \times D \times T$, где $t \in T$ -- латентная переменная, представляющая тему. \parencite{vorontsov2015topic}

Метод тематического моделирования опирается на следующие допущения:
\begin{enumerate}
  \item вхождение термина в документ некоторым образом связано с темой;
  \item документы рассматриваются как неупорядоченные наборы терминов, порядок слов и грамматичность предложений игнорируются (гипотеза ``мешка слов'');
  \item порядок текстов в коллекции не важен (гипотеза ``мешка документов'');
  \item встречаемость терминов в документах зависит от темы, но не зависит от документа, т.е. описывается общим для всех документов распределением $p(w|t)$ (гипотеза условной независимости). Это допущение имеет следующие эквивалентные представления: $p(w|d,t) = p(w|t)$, $p(d|w,t) = p(d|t)$, $p(d, w|d) = p(d|t)p(w|t)$.
\end{enumerate}

Порождение документов на основе множества латентных тем может быть формализовано с помощью следующей генеративной модели: $p(w|d) = \sum_{t \in T}p(w|t)p(t|d)$. Эта модель выражает вероятность появления термина в документе через известные распределения $p(w|t)$ и $p(t|d)$. Построение тематической модели является обратной задачей: здесь по известной текстовой коллекции $D$ требуется найти распределения $p(w|t)$ и $p(t|d)$. В общем виде задача определяется как оптимизационная задача стохастического матричного разложения, состоящая в нахождении двух матриц $\Phi = (\phi_{wt})_{W \times T}$ (матрицы терминов тем) и $\Theta = (\theta_{td})_{T \times D}$ (матрицы тем документов), таких что матрица частот терминов в документах $F=(f_{wd})_{W \times D}, f_{wd} = n_{dw} / n_d$ аппроксимируется произведением $F \approx \Phi \Theta$. При этом матрицы $\Phi$ и $\Theta$ рассматриваются в качестве параметров модели, и все три матрицы $F$, $\Phi$ и $\Theta$ являются \textit{стохастическими}, т.е. столбцы $f_d, \phi_t$ и $\theta_d$ представляют собой дискретные распределения (матрицы имеют неотрицательные компоненты и нормированы по столбцам).

В общем виде задача построения тематической модели является некорректно поставленной, т.к. допускает бесконечное множество решений. Для получения устойчивого решения требуется введение ряда дополнительных критериев и ограничений (например, предположение о том, что столбцы $\theta_d$ и $\phi_t$ являются случайными векторами, порождаемыми распределением Дирихле, в модели латентного размещения Дирихле). Число и разнообразие таких критериев, а также различные варианты используемых методов оценки параметров, определяют значительное число вариантов практических реализаций данного подхода. В числе особенностей метода, имеющих важное значение для практики, следует отметить произвольный выбор числа извлекаемых тем пользователем (хотя существуют методы, позволяющие устранять коррелированные и малоинформативные темы), возможные затруднения с интерпретируемостью извлекаемых тем.

Приложения тематического моделирования включают информационный поиск (information retrieval), в особенности разведочный поиск (exploratory search), выявление трендов в новостях и научных публикациях, анализ социальных сетей, классификацию и категоризацию документов, суммаризацию, сегментацию текстов, тегирование веб-страниц и новостей, обнаружение спама, разработку рекомендательных систем и пр. \parencite{vorontsov2015topic}


\section{Методы оценки качества результатов}

Методы оценки качества результатов тематического моделирования можно разделить на внутренние (intrinsic) и внешние (extrinsic). В первом случае речь идет об оценке непосредственно модели с точки зрения её характеристик, осуществляемой по исходной текстовой коллекции. В числе метрик такого рода: перплексия (perplexity), разреженность матриц $\Phi$ и $\Theta$ (количество компонентов, близких к нулю), чистота и контрастность тем, когерентность наиболее вероятных слов темы (общепринятая мера интерпретируемости темы), доля фоновых слов и др. Во втором случае оценка осуществляется с точки зрения качества целевой системы, в которой результаты тематического моделирования используются как составной компонент (например, оценка качества информационного поиска с привлечением ассессоров).

Далее кратко опишем распространенные внутренние оценки.

\subsection*{Перплексия}

Перплексия (perplexity) традиционно используется для характеристики языковых моделей\footnote{Языковая модель -- это распределение $p(w), w \in L \subseteq \Sigma^*$, где $\Sigma^*$ -- множество всевозможных последовательностей букв из конечного алфавита $\Sigma$.}
и характеризует несоответствие модели $p(w)$ наблюдаемому слову $w$. Определяется через логарифм функции правдоподобия и применительно к задаче тематического моделирования формулируется следующим образом: $\mathcal{P}(D; p) = exp(-\frac{1}{n}L(\Phi, \Theta)) = exp(-\frac{1}{n}\sum_{d \in D}\sum_{w \in W}n_{dw}ln\,p(w|d))$, где $n$ -- число терминов в коллекции. Модель $p$ тем лучше предсказывает термины в документах коллекции, чем меньше перплексия.

При обучении тематической модели на всей коллекции и вычислении перплексии на той же обучающей выборке, перплексия оказывается занижена (т.е. оказывается лучше, чем должна быть) в силу переобучения. Чтобы этого избежать, рассматривают перплексию контрольной выборки (hold-out perplexity), т.е. считают на части коллекции, не использовавшейся для обучения (например, отбирая случайным образом десятую часть документов из исходной коллекции).

Перплексия как метрика обладает определенными недостатками, т.к. её значение зависит не только от качества модели, но и от длины документов, размера словаря и т.п.

\subsection*{Когерентность}

Когерентность (согласованность) темы определяется на основе поточечной взаимной информации (pointwise mutual information) и характеризует совместную встречаемость наиболее вероятных терминов определенной темы в документах коллекции (использовавшейся для обучения или сторонней).

$PMI(t) = \sum_{i=1}^{k-1}\sum_{j=i+1}^{k}log\frac{N(w_i, w_j)}{N(w_i)N(w_j)}$, где $w_i$ - i-й термин в порядке убывания $\phi_{wt}$, $N(w)$ -- число документов, содержащих термин $w$, $N(w, w')$ -- число документов, в которых термины $w$ и $w'$ встречаются в окне фиксированного размера, $k$ -- ограничение на число наиболее вероятных терминов, используемых для расчета, на практике обычно равно 10.

Средняя когерентность тем применяется в качестве меры интерпретируемости тематической модели.

\subsection*{Разреженность}

Разреженность -- доля ненулевых элементов в матрицах $\Phi$ и $\Theta$ (либо только по предметным темам, если в модели различаются предметные и фоновые темы).

\subsection*{Показатели интерпретируемости темы на основе лексического ядра}

Множество слов, отличающих одну тему от остальных, называют её лексическим ядром. Наличие ядра позволяет характеризовать тему как интерпретируемую.

Ядро темы $W_t = \{w \in W \; | \; p(t|w) > 0,25\}$, где $p(t|w) = \phi_{wt}\frac{n_t}{n_w}$. Показатели интерпретируемости темы:
\begin{enumerate}
    \item чистота $\Sigma_{w \in W_t}p(w|t)$ (чем выше, тем лучше)
    \item контрастность $\frac{1}{|W_t|\Sigma_{w \ in W_t}p(t|w)}$ (чем выше, тем лучше)
    \item мощность ядра $|W_t|$ (оптимум вблизи $\frac{|W|}{|T|}$)
\end{enumerate} 

Соответствующие показатели для всей модели считаются как среднее по всем предметным темам.

\subsection*{Доля фоновых слов}

$\mathcal{B} = \frac{1}{n}\Sigma_{d \in D}\Sigma_{w \in W}\Sigma_{t \in T}n_{dw}p(t|d,w)$, принимает значения в диапазоне $[0, 1]$, экстремальные значения говорят о вырожденности модели.


\section{Основные традиционные методы (pLSA, LDA)}

Основные традиционные методы: байесовское обучение, LSA, pLSA, LDA. Обзор основных реализаций (Gensim, Vowpal Wabbit, BigARTM и др.)

\section{Аддитивная регуляризация для тематических моделей (ARTM)}

Понятие регуляризации. Аддитивная регуляризация. ARTM. Характеристика метода. Технические аспекты (отсутствие автоматической процедуры подбора регуляризаторов и пр.). Реализация метода в библиотеке BigARTM.

\section{Нейросетевые методы. BERTopic}

Нейросетевые методы. Отказ от допущения ``мешка слов'', попытки учесть контекстные значения. Краткая характеристика основных механизмов трансформерных моделей: вектроные репрезентации значений слов, энкодер-декодер архитектура, механизм внимания, KVQ-формализм, понятие языковой модели и маскированной языковой модели (MLM). Понятие предобученной модели и transfer learning. BERT как энкодер, способный учитывать контекстные значения слов. Тематическое моделирование с помощью архитектуры BERTopic.

\section{Состязательное тематическое моделирование}

Состязательные сети (adversarial). Тематическое моделирование средствами adversarial topic modeling (ATM).



\chapter{Автоматическое именование тем}

\section{Характеристика проблемы интерпретируемости}

Характеристика проблемы интерпретируемости тем (не во всех downstream задачах это является проблемой). В тематическом моделировании документы представляются как вектора, где размерностями выступают темы, а значениями (коэффициентами) по соответствующим размерностям -- вероятности тем. Т.о. это представление является интерпретируемым (в отличие от document embeddings в doc2vec и аналогичных подходах). Однако, число тем и их содержание уже не являются непосредственно интерпретируемыми: число тем задается пользователем (ср. k-NN кластеризация), темы описываются распределением над термами.

Задача интерпретации тем пользователем первоначально решалась либо представлением темы в виде списка наиболее важных слов, либо путем её аннотирования вручную. Впервые задача автоматического именования тем была поставлена в работе \parencite{mei2007automatic} и в дальнейшем стала важной подзадачей направления тематического моделирования. В данном разделе рассмотрим основные подходы к её решению.

\section{Mei, Shen, Zhai. Automatic labeling of multinomial topc models. KDD, 2007.}

Первая работа, в которой ставится задача автоматического именования тем -- \parencite{mei2007automatic}. Здесь предлагается формальная постановка задачи, рассматриваются интуитивные требования к выбору наименований тем и разрабатываются формальные критерии, отражающие различные аспекты интерпретируемости. В этой работе предлагается двухэтапный подход, ставший в дальнейшем основным при решении задачи автоматического именования тем, состоящий в генерации набора  наименований-кандидатов и их последующего ранжирования с целью отбора наилучших по ряду критериев, включающих понятность для пользователя, способность отражать значение темы и возможность отличать одну тему от остальных, опираясь на её наименование.

В тематическом моделировании обнаруживаемые темы представляются в виде мультиномиального распределения на множестве слов (т.е. в виде униграммной языковой модели). Ключевая идея подхода, предлагаемого в статье, состоит в представлении возможных наименований также в виде мультиномиального распределения на множестве слов. Благодаря этому оказывается возможным оценивать сходство двух этих распределений и таким образом свести задачу выбора наилучших наименований к оптимизационной задаче, основанной на минимизации расстояния Кульбака-Лейблера между распределениями темы и наименования.

Среди возможных вариантов выбора кандидатов для наименований, включающих отдельные термины, фразы или предложения, авторы останавливаются на использовании фраз, т.к. фразы отличаются большей согласованностью, чем отдельные термины, достаточно компактны по сравнению с предложениями, но при этом способны отражать значение темы. Кроме того, было замечено, что при ручной разметке, аннотаторы также используют коротки фразы для описания значения тем.

Предлагается следующая формальная постановка задачи автоматического наименования тем:

Тематическая модель $\theta$ текстовой коллекции $\mathcal{C}$ -- это вероятностное распределение на множестве слов $\{p(w|\theta)\}_{w \in V}$, где $V$ -- словарь. Ясно, что $\sum_{w \in V}p(w|\theta) = 1$.

Наименование $l$ для темы $\theta$ -- это осмысленная последовательность слов, отражающая скрытое значение $\theta$.

Оценка релевантности наименования темы $s(l, \theta)$ является характеристикой семантичекого сходства между наименованием и темой. Наименование $l_1$ лучше наименования $l_2$, если $s(l_1, \theta) > s(l_2, \theta)$.

\textit{Задача наименования отдельной темы} $\theta$, извлеченной из текстовой коллекции, состоит в определении множества наименований-кандидатов $L = \{l_1, \ldots, l_m\}$, разработке функции оценки релевантности $s(l_i, \theta)$ и нахождении подмножества $L_\theta = \{l_{\theta, 1}, \ldots, l_{\theta, n}\}$ из $n$ наименований, имеющих наибольшую оценку релевантности  для $\theta$.

Задача наименования отдельной темы обобщается на \textit{множество тем} следующим образом. Пусть $\Theta = \{\theta_1, \ldots, \theta_k\}$ -- множество из $k$ тем, $L = \{l_1, \ldots, l_m\}$ -- множество наименований-кандидатов. Требуется отобрать подмножество из $n_i$ наименований, $L_{\theta_i} = \{l_{\theta_i, 1}, \ldots, l_{\theta_i, n}\}$ для каждой темы $\theta_i \in \Theta$ .

В некоторых практических сценариях применения тематического моделирования набор наименований-кандидатов может быть заранее определен (например, задаваться онтологией). Однако, в общем случае предполагается, что такой набор заранее неизвестен. Его задание опирается на использование некоторой референсной текстовой коллекции, которая может быть внешней по отношению к набору документов, для которых создается тематическая модель, либо совпадать с ним.\footnote{В качестве примера внешней референсной коллекции можно привести статьи с конференции по определенной тематике.}

Общая процедура нахождения наилучших наименований для тем включает три этапа:
\begin{enumerate}
    \item создается набор осмысленных наименований-кандидатов 
    \item разрабатывается функция оценки релевантности наименований относительно темы
    \item производится отбор наименований для удовлетворения интуитивных критериев качества наименования, таких как способность различение тем и адекватное отражение семантики отдельной темы. 
\end{enumerate}

В качестве методов порождения набора наименований-кандидатов рассматриваются неглубокий синтаксический анализ (chunking) и статистические методы отбора n-грамм. Первый метод основан на применении специальных программных решений, извлекающих именные группы с опорой на частеречную разметку, определенный грамматический формализм и т.п. Эти решения сложны, могут требовать дополнительных обучающих данных и доступны для ограниченного числа языков. Второй метод опирается на отбор частотных n-грамм из референсной текстовой коллекции в предположении, что такие n-граммы обычно представляют собой осмысленные фразы. Отбор производится с помощью статистических метрик  (например, взаимной информации) или с помощью методов статистической проверки гипотез ($\chi^2$, $t$-тест Стьюдента). Этот способ применим к текстовым коллекциям в любых предметных областях, однако показывает хорошие результаты только для биграмм.

Для оценки семантической релевантности наименований-кандидатов извлекаемым темам предлагаются две функции.

Функция оценки релевантности \textit{нулевого порядка} позволяет оценивать релевантность наименования теме без использования референсной текстовой коллекции. Обозначим фразу-кандидата $l = u_0 u_1 \ldots u_m$, где $u_i$ -- слово. Оценка $score = log \, \frac{p(l|\theta)}{p(l)} = \sum_{0 \leqslant i \leqslant m} log \, \frac{p(u_i|\theta)}{p(u_i)}$, где $u_i$ предполагаются независимыми. Интуитивный смысл данной функции: фраза, содержащая более важные слова по распределению темы (т.е. с большим $p(w|\theta)$), является хорошим наименованием для данной темы. Здесь вероятность слова $p(u_i)$ в знаменателе требуется для компенсации смещения, связанного с обучающей текстовой коллекцией, и может оцениваться по вспомогательной коллекции или быть принята за равномерную. То есть, с помощью данной функции фраза оценивается на основе правдоподобия её порождения тематической моделью $\theta$ по сравнению с некоторым вспомогательным распределением.

Функция оценки релевантности \textit{первого порядка} основана на представлении наименования с помощью распределения над словами исходной текстовой коллекции, благодаря чему оно может сравниваться с распределением темы по расстоянию Кульбака-Лейблера. Пусть $l$ -- наименование, $\{p(w|l)\}$ -- мультиномиальное распределение, ассоциированное с $l$. Тогда расстояние Кульбака-Лейблера $D(\theta||l)$ отражает то, насколько хорошо $l$ описывает $\theta$.  $D(\theta||l) = 0$ в случае, если эти распределения совпадают. Построение распределения $\{p(w|l)\}$ требует использования референсной текстовой коллекции $\mathcal{C}$, выступающей в качестве контекста. Функция оценки релевантности наименования $l$ относительно темы $\theta$ определяется как отрицательное расстояние Кульбака-Лейблера между распределениями $\{p(w|\theta)\}$ и $\{p(w|l)\}$. За счет введения контекста $\mathcal{C}$, функция оценки релевантности может быть преобразована следующим образом:

$score(l, \theta) = -D(\theta||l) =$

$= -\sum_{w}p(w|\theta)log\,\frac{p(w|\theta)}{p(w|l)} =$

$= -\sum_{w}p(w|\theta)log\,[\frac{p(w|\mathcal{C})}{p(w|l,\mathcal{C})}\cdot  \frac{p(w|\theta)}{p(w|\mathcal{C})}\cdot\frac{p(w|l,\mathcal{C})}{p(w|l)}] =$

$= -\sum_{w}p(w|\theta)log\,\frac{p(w|\mathcal{C})}{p(w|l,\mathcal{C})} -\sum_{w}p(w|\theta)log\,\frac{p(w|\theta)}{p(w|\mathcal{C})} -\sum_{w}p(w|\theta)log\,\frac{p(w|l,\mathcal{C})}{p(w|l)} =$

$= \sum_{w}p(w|\theta)\,PMI(w,l|\mathcal{C}) - D(\theta||\mathcal{C}) + Bias(l, \mathcal{C})$

Здесь второй компонент представляет собой расстояние Кульбака-Лейблера между темой и \textit{контекстом}. Т.к. он не зависит от наименований, в функции оценки релевантности наименований он может быть опущен. Третий компонент может рассматриваться как смещение (bias) при использовании контекста для оценки семантической релевантности $l$ и $\theta$. Наконец, первый компонент представляет собой матожидание поточечной взаимной информации (PMI) между $l$ и терминами темы при заданном контексте: $E_\theta(PMI(w,l|\mathcal{C}))$.

При отсутствии априорных данных о смещении, определяемом контекстом (т.е. референсной текстовой коллекцией), функция оценки релевантности может быть редуцирована до $s(l,\theta) = E_\theta(PMI(w,l|\mathcal{C}))$. Значения $E_\theta(PMI(w,l|\mathcal{C}))$ могут считаться независимо от тем. В случае, если определенное слово из обучающей текстовой коллекции не содержится в референсной коллекции, $PMI(w,l|\mathcal{C})$ не определена. Тогда соответствующее слагаемое может полагаться равным нулю, либо может применяться та или иная стратегрия сглаживания (например, сглаживание Лапласа). 

Данная функция берется в качестве искомой функции оценки релевантности первого порядка и используется для ранжирования наименований-кандидатов. Интуитивно, она тем выше ранжирует наименование, чем сильнее его семантическая связь с наиболее важными (вероятными) словами темы.

Заключительная часть процедуры нахождения наилучших наименований тем состоит в отборе таких наименований, которые удовлетворяют интуитивным критериям качества. 

Во-первых, необходимо, чтобы наименования максимально отражали семантическую информацию конкретной темы. Поскольку для описания темы может применяться более одной фразы, необходимо последовательно максимизировать информативность каждого следующего наименования с учетом уже выбранных. Это достигается путем поочередного отбора наименований из числа кандидатов, при этом на каждом шаге максимизируется критерий максимальной маржинальной релевантности (maximal marginal relevance, MRR):

\[\hat{l} = \argmax_{\l \in L\setminus S} \, [\lambda Score(l, \theta) - (1-\lambda)\max_{l' \in S} Sim(l', l)], \]

\noindent где $S$ -- множество уже отобранных наименований, $Sim(l', l) = -D(l'||l) = -\Sigma_w p(w|l') log \frac{p(w|l')}{p(w|l)}$, $\lambda$ -- (гипер)параметр.

Во-вторых,


\section{Методы с опорой на пользовательские аннотации и др.}

Lorem ipsum dolor sit amet



\chapter{Автоматическое определение однородности}

\section{Метрики однородности корпуса и темы}

Метрики однородности корпуса (corpus homogeneity) vs. метрики однородности темы (topic homogeneity). Связь метрик однородности корпуса и метрик качества тематического моделирования на примере perplexity. Использование тематических признаков (topical features) слов в задачах снятия лексической неоднозначности (WSD) и др.

\section{BoW}

Метрики однородности корпуса на основе частот слов (статистики на bag-of-words)

\section{Thesauri}

Метрики однородности корпуса на основе онтологий/тезаурусов – анализ родственных слов в иерархии
семантических понятий

\section{Embeddings}

Метрики однородности корпуса на основе расстояний между эмбеддингами текстов

\section{Enthropy}

Метрики однородности корпуса на основе энтропии
