\chapter*{Введение}
\addcontentsline{toc}{chapter}{Введение}

План работы\\

Раздел 1. Задача тематического моделирования и методы решения.
\begin{itemize}
  \item Задача тематического моделирования. Формальное определение. Основные допущения и гипотезы, их обсуждение (``мешок слов'', ``мешок документов''). Приложения и downstream задачи, прикладные особенности (произвольный выбор числа извлекаемых тем пользователем, затруднения с интерпретируемостью извлекаемых тем).
  \item Методы оценки качества результатов тематического моделирования. Intrinsic vs. extrinsic evaluation. Метрики: perplexity, разреженность матриц $\Phi$ и $\Theta$ (в BigARTM), чистота и контрастность тем, когерентность наиболее вероятных слов темы (общепринятая мера интерпретироуемости темы), доля фоновых слов (lower is better)  и др.
  \item Основные традиционные методы: байесовское обучение, LSA, pLSA, LDA. Обзор основных реализаций (Gensim, Vowpal Wabbit, BigARTM и др.)
  \item Понятие регуляризации. Аддитивная регуляризация. ARTM. Характеристика метода. Технические аспекты (отсутствие автоматической процедуры подбора регуляризаторов и пр.). Реализация метода в библиотеке BigARTM.
  \item Нейросетевые методы. Отказ от допущения ``мешка слов'', попытки учесть контекстные значения. Краткая характеристика основных механизмов трансформерных моделей: вектроные репрезентации значений слов, энкодер-декодер архитектура, механизм внимания, KVQ-формализм, понятие языковой модели и маскированной языковой модели (MLM). Понятие предобученной модели и transfer learning. BERT как энкодер, способный учитывать контекстные значения слов. Тематическое моделирование с помощью архитектуры BERTopic.
  \item Состязательные сети (adversarial). Тематическое моделирование средствами adversarial topic modeling (ATM).
\end{itemize}

\bigskip

Раздел 2. Интерпретируемость тематического моделирования
\begin{itemize}
  \item Характеристика проблемы интерпретируемости тем (не во всех downstream задачах это является проблемой). В тематическом моделировании документы представляются как вектора, где размерностями выступают темы, а значениями (коэффициентами) по соответствующим размерностям -- вероятности тем. Т.о. это представление является интерпретируемым (в отличие от document embeddings в doc2vec и аналогичных подходах). Однако, число тем и их содержание уже не являются непосредственно интерпретируемыми: число тем задается пользователем (ср. k-NN кластеризация), темы описываются распределением над термами. 
  \item Задача автоматической разметки тем как подзадача тематического меоделирования. Обзор подходов к её решению.
  \item Методы, основанные на графовых представлениях
  \item Методы с опорой на пользовательские аннотации
  \item и др.
\end{itemize}

\bigskip

Раздел 3. Автоматическое определение однородности

\begin{itemize}
    \item Метрики однородности корпуса (corpus homogeneity) vs. метрики однородности темы (topic homogeneity). Связь метрик однородности корпуса и метрик качества тематического моделирования на примере perplexity. Использование тематических признаков (topical features) слов в задачах снятия лексической неоднозначности (WSD) и др.
    \item Метрики однородности корпуса на основе частот слов (статистики на bag-of-words)
    \item Метрики однородности корпуса на основе онтологий/тезаурусов – анализ родственных слов в иерархии
    семантических понятий
    \item Метрики однородности корпуса на основе расстояний между эмбеддингами текстов
    \item Метрики однородности корпуса на основе энтропии
\end{itemize}